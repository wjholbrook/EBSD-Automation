{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "from matplotlib.image import NonUniformImage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "from scipy.stats import norm, lognorm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import distance, Voronoi, voronoi_plot_2d, ConvexHull, Delaunay\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import random\n",
    "import sys\n",
    "from itertools import combinations \n",
    "import time\n",
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "import sobol_seq\n",
    "import os,glob\n",
    "\n",
    "#plotting style\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_style('white')\n",
    "\n",
    "def js_divergence_scipy(hist1,hist2):\n",
    "    return (distance.jensenshannon(hist1, hist2, base=2))**2\n",
    "\n",
    "def nearest_neighbors(values, all_values, nbr_neighbors=1):\n",
    "    nn = NearestNeighbors(nbr_neighbors, metric='euclidean', algorithm='kd_tree').fit(all_values)\n",
    "    dists, idxs = nn.kneighbors(values)\n",
    "    return idxs\n",
    "\n",
    "def golden(n,d=2):\n",
    "    g = 1.32471795724474602596 \n",
    "    alpha = np.zeros(d) \n",
    "    for j in range(d): \n",
    "        alpha[j] = pow(1/g,j+1) %1 \n",
    "    z = np.zeros((n, d)) \n",
    "    seed=0.5\n",
    "    for i in range(n): \n",
    "        z[i] = (seed + alpha*(i+1)) %1 \n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get area of each voronoi region\n",
    "def voronoi_volumes(points, percent=100, window_vol=False):\n",
    "    v = Voronoi(points)\n",
    "    #create zeros of len points\n",
    "    vol = np.zeros(v.npoints)\n",
    "    #create vol index and get index of voronoi region for input point \n",
    "    for i, reg_num in enumerate(v.point_region):\n",
    "        #get indices of each region\n",
    "        indices = v.regions[reg_num]\n",
    "        #-1 means vertex outside diagram\n",
    "        if -1 in indices: # some regions can be opened\n",
    "            #ignore infinite regions\n",
    "            vol[i] = 0\n",
    "            \n",
    "        else: #can compute area, use convex hull\n",
    "            vert=v.vertices[indices]\n",
    "            vert[np.where(vert<0)]=0\n",
    "            max_vert=math.ceil(np.amax(raw_data[:,3]))       \n",
    "            y_max_vert=math.ceil(np.amax(raw_data[:,4]))\n",
    "            if y_max_vert > max_vert:\n",
    "                max_vert = y_max_vert\n",
    "            \n",
    "            if window_vol == False:\n",
    "                vert[np.where(vert>max_vert)]=max_vert\n",
    "            else:\n",
    "                vert[np.where(vert>(max_vert*percent))]=(max_vert*percent)\n",
    "            \n",
    "            try:\n",
    "                vol[i] = ConvexHull(vert).volume\n",
    "            except: #convex hull error, exclude area\n",
    "                vol[i] = 0\n",
    "    #can use grain IDs, since areas match input point\n",
    "    return vol\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def new_vor_p(num_bins=25):\n",
    "#     #get areas\n",
    "#     vol=voronoi_volumes(coords)   \n",
    "#     #turn area into percent of total area\n",
    "#     norm_vol=(vol)/sum(vol)\n",
    "#     #multiply by total number of pixels to find how many pixels in each area\n",
    "#     scale_vol=norm_vol*len(raw_data)\n",
    "#     #round to nearest whole number\n",
    "#     round_vol=np.round(scale_vol,decimals=0)\n",
    "#     #create array, col1 is ID, col2 is area\n",
    "#     grain_IDs_and_areas=np.stack((grain_IDs,round_vol),axis=1)\n",
    "#     #get unique IDs and their count\n",
    "#     unique_IDs,unique_IDs_count=np.unique(grain_IDs_and_areas[:,0],return_counts=True)\n",
    "#     #create array to hold unique IDs and their sizes added together\n",
    "#     unique_grain_IDs_and_total_areas=np.stack((unique_IDs,np.zeros(len(unique_IDs))),axis=1)\n",
    "    \n",
    "#     #construct IDs and count\n",
    "#     for row in unique_grain_IDs_and_total_areas:\n",
    "#         grain=grain_IDs_and_areas[np.where(grain_IDs_and_areas[:,0] == row[0])]\n",
    "#         total=np.sum(grain[:,1])\n",
    "#         row[1]=total\n",
    "    \n",
    "#     #double edge grains\n",
    "#     row_ID=0\n",
    "#     for row in unique_grain_IDs_and_total_areas:\n",
    "#         #if the grain ID is an edge\n",
    "#         if np.isin(row[0],edge_grains[:,0]):\n",
    "#             #double the total grain area\n",
    "#             unique_grain_IDs_and_total_areas[row_ID,1]*=2\n",
    "#         row_ID+=1\n",
    "        \n",
    "#     #return count for hist\n",
    "#     count = 0.138*unique_grain_IDs_and_total_areas[:,1]\n",
    "#     q, q_bin_edges = np.histogram(count, bins=num_bins, range=(0.01,50), density=True)\n",
    "#     q = np.append(q, 0)\n",
    "#     #mean,var \n",
    "#     total_mean=np.mean(count)\n",
    "#     var=np.var(count)\n",
    "    \n",
    "#     return q, total_mean, var\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def grain_areas(sampled_coords,sampled_IDs,percent=100,num_bins=25,window_vol=False,return_p=False,numerical=False):   \n",
    "    global p\n",
    "    global edge_grains\n",
    "#     plt.scatter(sampled_coords[:,0],sampled_coords[:,1],c=sampled_IDs,cmap='prism')\n",
    "    #get areas\n",
    "    vol=voronoi_volumes(sampled_coords,window_vol)\n",
    "    #turn area into percent of total area\n",
    "    norm_vol=(vol)/sum(vol)\n",
    "    #multiply by total number of pixels to find how many pixels in each area\n",
    "    if window_vol==False:\n",
    "        scale_vol=norm_vol*len(raw_data)\n",
    "    else:\n",
    "        scale_vol=norm_vol*len(sampled_coords)\n",
    "    #round to nearest whole number\n",
    "    round_vol=np.round(scale_vol,decimals=0)\n",
    "    #create array, col1 is ID, col2 is area\n",
    "    grain_IDs_and_areas=np.stack((sampled_IDs,round_vol),axis=1)\n",
    "    #get unique IDs and their count\n",
    "    unique_IDs,unique_IDs_count=np.unique(grain_IDs_and_areas[:,0],return_counts=True)\n",
    "    #create array to hold unique IDs and their sizes added together\n",
    "    unique_grain_IDs_and_total_areas=np.stack((unique_IDs,np.zeros(len(unique_IDs))),axis=1)\n",
    "    \n",
    "    #construct IDs and count\n",
    "    for row in unique_grain_IDs_and_total_areas:\n",
    "        grain=grain_IDs_and_areas[np.where(grain_IDs_and_areas[:,0] == row[0])]\n",
    "        total=np.sum(grain[:,1])\n",
    "        row[1]=total\n",
    "        \n",
    "    #create window edges\n",
    "    if window_vol==True:\n",
    "        #create array to hold grain IDs and edge column\n",
    "        edges=np.zeros((len(sampled_IDs),1))\n",
    "        grain_slice_and_edges=np.concatenate((sampled_IDs.reshape(-1,1),edges.reshape(-1,1)),axis=1)\n",
    "\n",
    "        #min x\n",
    "        left_edge=np.amin(sampled_coords[:,0])\n",
    "        #max x\n",
    "        right_edge=np.amax(sampled_coords[:,0])\n",
    "        #min y\n",
    "        bottom_edge=np.amin(sampled_coords[:,1])\n",
    "        #max y\n",
    "        top_edge=np.amax(sampled_coords[:,1])\n",
    "        #assign edge grains\n",
    "        index=0\n",
    "        for row in sampled_coords:\n",
    "            #x values\n",
    "            if row[0] <= left_edge or row[0] >= right_edge:\n",
    "                #it's an edge\n",
    "                grain_slice_and_edges[index,1]=1\n",
    "            #y values\n",
    "            if row[1] <= bottom_edge or row[1] >= top_edge:\n",
    "                #it's an edge\n",
    "                grain_slice_and_edges[index,1]=1\n",
    "            index+=1 \n",
    "    \n",
    "        #slice array for edge calculation\n",
    "        edge_grains=grain_slice_and_edges[np.where(grain_slice_and_edges[:,1]==1)]\n",
    "        \n",
    "    #double edge grains\n",
    "    row_ID=0\n",
    "    for row in unique_grain_IDs_and_total_areas:\n",
    "        #if the grain ID is an edge\n",
    "        if np.isin(row[0],edge_grains[:,0]):\n",
    "            #double the total grain area\n",
    "            unique_grain_IDs_and_total_areas[row_ID,1]*=2\n",
    "        row_ID+=1\n",
    "    \n",
    "    count = 0.138*unique_grain_IDs_and_total_areas[:,1]\n",
    "\n",
    "    #**************************************\n",
    "    \n",
    "    count=count[np.where(count>0)]\n",
    "#     return count #!!!!!\n",
    "    mean=np.mean(count)\n",
    "    var=np.var(count)\n",
    "    if numerical==True:\n",
    "        q, q_bin_edges = np.histogram(count, bins=num_bins, range=(0.01,50), density=True)\n",
    "    else: #area\n",
    "        q, q_bin_edges = np.histogram(count, bins=num_bins, range=(0.01,50), density=True, weights=count)\n",
    "    q = np.append(q, 0)\n",
    "    if return_p==True:\n",
    "        return q, mean, var\n",
    "    else:\n",
    "        return js_divergence_scipy(p,q), mean, var\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def window_slices():\n",
    "    x_min=np.amin(raw_data[:,3])\n",
    "    x_max=np.amax(raw_data[:,3])\n",
    "    y_min=np.amin(raw_data[:,4])\n",
    "    y_max=np.amax(raw_data[:,4])\n",
    "    #window one (bottom left)\n",
    "    raw_data_slice_1 = raw_data[np.where((raw_data[:,3] <= x_max/2) & (raw_data[:,4] <= y_max/2))]\n",
    "\n",
    "    # #window two (bottom right)\n",
    "    raw_data_slice_2 = raw_data[np.where((raw_data[:,3] >= x_max/2) & (raw_data[:,4] <= y_max/2))]\n",
    "    raw_data_slice_2[:,3] -= np.min(raw_data_slice_2[:,3])\n",
    "\n",
    "    # #window three (top left)\n",
    "    raw_data_slice_3 = raw_data[np.where((raw_data[:,3] <= x_max/2) & (raw_data[:,4] >= y_max/2))]\n",
    "    raw_data_slice_3[:,4] -= np.min(raw_data_slice_3[:,4])\n",
    "\n",
    "    # #window four (top right)\n",
    "    raw_data_slice_4 = raw_data[np.where((raw_data[:,3] >= x_max/2) & (raw_data[:,4] >= y_max/2))]\n",
    "    raw_data_slice_4[:,3] -= np.min(raw_data_slice_4[:,3])\n",
    "    raw_data_slice_4[:,4] -= np.min(raw_data_slice_4[:,4])\n",
    "    \n",
    "    return [raw_data_slice_1, raw_data_slice_2, raw_data_slice_3, raw_data_slice_4]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample(size,method):   \n",
    "    global raw_data\n",
    "    global coords\n",
    "    global grain_IDs\n",
    "    global grain_IDs_and_edges\n",
    "    global coords_and_grains\n",
    "    global coords_and_grains_copy\n",
    "    global edge_grains\n",
    "    global window_slice_list   \n",
    "    \n",
    "    if method == 'random':\n",
    "        #choose random points\n",
    "        random_coords_and_grains=coords_and_grains[np.random.choice(coords_and_grains.shape[0], size, replace=False), :]\n",
    "        sliced_coords=random_coords_and_grains[:,0:2]\n",
    "        sliced_grain_IDs=random_coords_and_grains[:,2]\n",
    "    \n",
    "    if method == 'square':\n",
    "        phi = (np.sqrt(5)+1)/2\n",
    "        ratio = np.sqrt(3)/2 # cos(60°)\n",
    "        coords = raw_data[:,3:5]\n",
    "        N = size\n",
    "        \n",
    "        N_X = int(np.sqrt(N))\n",
    "        N_Y = N // N_X\n",
    "        xv, yv = np.meshgrid(np.arange(N_X), np.arange(N_Y), sparse=False, indexing='xy')\n",
    "        square_coords=np.concatenate((xv.reshape(-1,1), yv.reshape(-1,1)), axis=1)\n",
    "        square_coords[:,0] = square_coords[:,0] * (np.amax(raw_data[:,3]) / np.amax(square_coords[:,0]))\n",
    "        square_coords[:,1] = square_coords[:,1] * (np.amax(raw_data[:,4]) / np.amax(square_coords[:,1]))\n",
    "        grid_sample=raw_data[nearest_neighbors(square_coords,coords),3:6]\n",
    "        grid_sample=grid_sample[:,0]\n",
    "        \n",
    "        sliced_coords=grid_sample[:,0:2]\n",
    "        sliced_grain_IDs=grid_sample[:,2]\n",
    "        \n",
    "    if method == 'hex':\n",
    "        phi = (np.sqrt(5)+1)/2\n",
    "        ratio = np.sqrt(3)/2 # cos(60°)\n",
    "        coords = raw_data[:,3:5]\n",
    "        N = size\n",
    "        \n",
    "        N_X = int(np.sqrt(N)/ratio)\n",
    "        N_Y = N // N_X\n",
    "        xv, yv = np.meshgrid(np.arange(N_X), np.arange(N_Y), sparse=False, indexing='xy')\n",
    "        xv = xv * ratio\n",
    "        xv[::2, :] += ratio/2\n",
    "        hex_coords=np.concatenate((xv.reshape(-1,1), yv.reshape(-1,1)), axis=1)\n",
    "        hex_coords[:,0] *= (np.amax(raw_data[:,3]) / np.amax(hex_coords[:,0]))\n",
    "        hex_coords[:,1] *= (np.amax(raw_data[:,4]) / np.amax(hex_coords[:,1]))\n",
    "        grid_sample=raw_data[nearest_neighbors(hex_coords,coords),3:6]\n",
    "        grid_sample=grid_sample[:,0]\n",
    "        \n",
    "        sliced_coords=grid_sample[:,0:2]\n",
    "        sliced_grain_IDs=grid_sample[:,2]\n",
    "        \n",
    "    if method == 'sobol':\n",
    "        #choose 2D sobol points\n",
    "        sobol = (sobol_seq.i4_sobol_generate(2, size))\n",
    "        sobol[:,0]*=np.amax(raw_data[:,3])\n",
    "        sobol[:,1]*=np.amax(raw_data[:,4])\n",
    "        sobol_sample=raw_data[nearest_neighbors(sobol,coords),3:6]\n",
    "        sobol_sample=sobol_sample[:,0]\n",
    "\n",
    "        sliced_coords=sobol_sample[:,0:2]\n",
    "        sliced_grain_IDs=sobol_sample[:,2]\n",
    "        \n",
    "    if method == 'gold':\n",
    "        #choose 2D golden points\n",
    "        golden_2D = golden(size)\n",
    "        golden_2D[:,0]*=np.amax(raw_data[:,3])\n",
    "        golden_2D[:,1]*=np.amax(raw_data[:,4])\n",
    "        golden_sample=raw_data[nearest_neighbors(golden_2D,coords),3:6]\n",
    "        golden_sample=golden_sample[:,0]\n",
    "\n",
    "        sliced_coords=golden_sample[:,0:2]\n",
    "        sliced_grain_IDs=golden_sample[:,2]\n",
    "        \n",
    "    if method == 'window':\n",
    "#         percent=sqrt(size/len(raw_data))\n",
    "        percent=sqrt(size/100000) #filesize\n",
    "    \n",
    "#         window_divergences=[]\n",
    "#         window_means=[]\n",
    "#         window_vars=[]\n",
    "        \n",
    "#         for raw_data_slice_x in window_slice_list:\n",
    "#             raw_data_slice=raw_data_slice_x\n",
    "    \n",
    "#             row_list=[]\n",
    "#             for index in range(0,len(raw_data_slice)):\n",
    "#                 if (raw_data_slice[index,3]<=int((np.amax(raw_data[:,3])*percent))) and (raw_data_slice[index,4]<=int((np.amax(raw_data[:,4])*percent))):\n",
    "# #                     row_list.append(raw_data_slice[index, 0:3]) #check this! angles?\n",
    "#                     row_list.append(raw_data_slice[index, 3:6])\n",
    "#                 else:\n",
    "#                     continue\n",
    "\n",
    "#             row_tuple=tuple(row_list)\n",
    "#             sliced_raw_data_slice=np.vstack(row_tuple)\n",
    "#             sliced_coords=sliced_raw_data_slice[:,0:2]\n",
    "#             sliced_grain_IDs=sliced_raw_data_slice[:,2]\n",
    "            \n",
    "#             if hist==True:\n",
    "#                 window_divergences.append(grain_areas(sliced_coords,sliced_grain_IDs,percent,window_vol=True,hist_bool=hist))\n",
    "#             else:\n",
    "#                 temp_mean, temp_var=grain_areas(sliced_coords,sliced_grain_IDs,percent,window_vol=True,hist_bool=hist)\n",
    "#                 window_means.append(temp_mean)\n",
    "#                 window_vars.append(temp_var)\n",
    "                \n",
    "#         if hist==True:\n",
    "#             #return average JS Divergence\n",
    "#             return sum(window_divergences)/4\n",
    "#         else:\n",
    "#             temp_mean_avg=sum(window_means)/4\n",
    "#             temp_var_avg=sum(window_vars)/4\n",
    "#             return temp_mean_avg, temp_var_avg\n",
    "\n",
    "        #Full Window\n",
    "        row_list=[]\n",
    "        for index in range(0,len(raw_data)):\n",
    "            if (raw_data[index,3]<=int((np.amax(raw_data[:,3])*percent))) and (raw_data[index,4]<=int((np.amax(raw_data[:,4])*percent))):\n",
    "                row_list.append(raw_data[index,3:6])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        row_tuple=tuple(row_list)\n",
    "        window_sample=np.vstack(row_tuple)\n",
    "        sliced_coords=window_sample[:,0:2]\n",
    "        sliced_grain_IDs=window_sample[:,2]\n",
    "        return grain_areas(sliced_coords,sliced_grain_IDs, window_vol=True)\n",
    "    \n",
    "    #return JS Divergence\n",
    "    return grain_areas(sliced_coords,sliced_grain_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
